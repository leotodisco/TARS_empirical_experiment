# ğŸ§ª Roadmap per la Valutazione In-Vitro di TARS

## **Fase 1: Progettazione dello Studio**

### 1. ğŸ¯ Definizione degli Obiettivi e Domande di Ricerca

- **RQ1:** Lâ€™uso di TARS migliora la comprensione del codice rispetto alla condizione di controllo?
- **RQ2:** Come viene percepito TARS dagli utenti in termini di utilitÃ , facilitÃ  dâ€™uso e carico cognitivo?
- **RQ3:** TARS riesce ad adattare le spiegazioni alle caratteristiche individuali dellâ€™utente (es. esperienza, intenzione, stile)?

---

### 2. ğŸ§ª Scelta del Design dello Studio

- **Design:** Within-subject
- **Condizioni:** Ogni partecipante completa due task (uno con TARS, uno senza)
- **Obiettivo:** Confronto diretto tra performance e percezioni dello stesso utente

---

### 3. ğŸ› ï¸ Preparazione degli Strumenti

- **Snippet di Codice:**  
  5 snippet Python (da repo open-source), selezionati per richiedere conoscenze di dominio (es. NLP, pandas). Rimuovi i commenti per aumentare la necessitÃ  di comprensione.

- **Quiz di Comprensione:**  
  Domande chiuse (max. 10 minuti).  
  Valutano comprensione semantica e funzionale dello snippet.

- **Questionari di Percezione:**  
  - **TAM** (Technology Acceptance Model)
  - **NASA-TLX** (Task Load Index)
  - **Feedback qualitativo** (opzionale)

---

## **Fase 2: Conduzione dello Studio**

### 1. ğŸ‘¥ Reclutamento dei Partecipanti

- **Target:** Studenti universitari con bassa esperienza di programmazione
- **Metodo:** Campionamento di convenienza (presentazioni, gruppi, contatti)
- **Pre-screening:**  
  - Programmazione (scala 1â€“10)  
  - FamiliaritÃ  con LLM (bassa/moderata/alta)

---

### 2. ğŸ“‹ Sessioni Sperimentali

- **Bilanciamento dei Gruppi:**  
  Ordine delle condizioni bilanciato per varianza su esperienza e LLM familiarity

- **Fasi della Sessione:**  
  - Briefing iniziale
  - Task 1 (TARS o controllo)
  - Quiz di comprensione
  - Task 2 (condizione opposta)
  - Quiz di comprensione
  - Survey post-esperimento

- **Tempi limite:**  
  - 20 min per ogni task  
  - 10 min per quiz

---

## **Fase 3: Analisi dei Dati**

---

### ğŸ” **Analisi della Comprensione del Codice (per RQ1)**

#### ğŸ“Œ Obiettivo

Valutare se TARS migliora la comprensione del codice rispetto alla condizione di controllo.

#### ğŸ§ª Variabili

- **Indipendente:** `Condition` (TARS vs Control)
- **Dipendenti:**  
  - QuizScore (0â€“100%)  
  - TaskTime (secondi)  
  - SelfComprehension (Likert 1â€“5)
- **Covariate:**  
  - ProgrammingExperience (1â€“10)

#### ğŸ”¬ Disegno

- Design within-subject
- Controbilanciamento ordine
- Due snippet bilanciati

#### ğŸ“ˆ Analisi Statistica

1. **Assunzioni**  
   - Shapiro-Wilk (normalitÃ )  
   - IQR per outlier su tempo

2. **Test**  
   - T-test appaiato o Wilcoxon

3. **Effetto**  
   - Cohenâ€™s d_z o r  
   - CI al 95% (bootstrap)

#### ğŸ“Š Regressione

```QuizScore ~ Condition + ProgrammingExperience + LLMFamiliarity + (1 | Participant)```

### Mixed-Effects Model

ğŸ§© Sottogruppi (opzionale)
	â€¢	Split in Bassa/Alta esperienza
	â€¢	Verifica interazioni Condition Ã— Experience

ğŸ“Œ Interpretazione
	â€¢	â†‘ QuizScore â†’ TARS migliora comprensione
	â€¢	â†“ TaskTime â†’ TARS piÃ¹ efficiente
	â€¢	Effetto su novizi â†’ supporto piÃ¹ utile ai meno esperti

ğŸ“¤ Output
	â€¢	Tabella comparativa
	â€¢	Violin/bar plot
	â€¢	Osservazioni soggettive

## ğŸ§  Analisi delle Percezioni (per RQ2)

### ğŸ“Œ Obiettivo
Indagare lâ€™esperienza utente in termini di:
- **UtilitÃ  percepita** (PU)
- **FacilitÃ  dâ€™uso** (PEOU)
- **Carico cognitivo** (NASA-TLX)

---

### ğŸ§ª Strumenti
- **TAM** â€“ Technology Acceptance Model (scala Likert 1â€“7)
- **NASA-TLX** â€“ Task Load Index (versione completa o semplificata)
- **Domande aperte** per raccogliere feedback qualitativo

---

### ğŸ§ª Somministrazione
- Dopo **ciascun task** (con e senza TARS)
- Survey **comparativa finale** al termine dellâ€™esperimento

---

### ğŸ“ˆ Analisi Statistica

1. **TAM (PU, PEOU)**
   - T-test appaiato oppure Wilcoxon signed-rank test (se non normale)

2. **NASA-TLX**
   - Calcolo del punteggio medio
   - Analisi per singole dimensioni: *Effort*, *Frustration*, ecc.

3. **Effetto**
   - Calcolo di p-value e **Cohenâ€™s dâ‚š**
   - Visualizzazione con boxplot o barre con **intervallo di confidenza (CI)**

---


### ğŸ“Œ Interpretazione
- **PU e PEOU alti** â†’ TARS Ã¨ percepito come utile e facile da usare
- **NASA-TLX piÃ¹ basso** â†’ TARS riduce il carico cognitivo rispetto al controllo

---

### ğŸ“¤ Output Attesi
- Tabelle comparate dei punteggi TAM & TLX
- Grafici comparativi (boxplot, barre con CI)
- Sintesi tematica dei commenti qualitativi

---

## ğŸ§© Analisi dellâ€™AdattivitÃ  (per RQ3)

### ğŸ“Œ Obiettivo
Verificare se TARS adatta le spiegazioni al profilo dellâ€™utente.

---

### ğŸ§ª Variabili

1. **Profilo Utente**
   - `ProgrammingExperience` (scala 1â€“10)
   - `LLMFamiliarity` (scala 1â€“5)

2. **Spiegazioni Generate da TARS**
   - Codifica secondo:
     - **Livello tecnico** (base, intermedio, avanzato)
     - **Tono** (formale, amichevole, didattico)
     - **Focalizzazione** (scopo generale vs dettagli tecnici)
     - **Formato** (paragrafo, elenco, misto)

3. **Percezione dellâ€™adattivitÃ **
   - Domanda Likert: *"La spiegazione era adatta al tuo livello?"*
   - Risposte aperte per motivare la valutazione

---

### ğŸ§© Codifica delle Spiegazioni
- Due valutatori indipendenti codificano un sottoinsieme
- Categorie:
  - Lessico usato (termini tecnici, semplificazioni)
  - ProfonditÃ  della spiegazione
  - Stile comunicativo (diretto, analogico, esemplificativo)
- Verifica coerenza tra **profilo utente** e **caratteristiche della spiegazione**

---

### ğŸ“ˆ Analisi Statistica

1. **Correlazioni**
   - **Spearmanâ€™s rho** tra:
     - Esperienza â†” livello tecnico della spiegazione
     - FamiliaritÃ  â†” formato/tono
     - Percezione â†” codifica oggettiva

2. **Analisi per gruppi**
   - Dividi partecipanti in *Principianti* vs *Esperti*
   - Confronta:
     - Spiegazioni ricevute
     - Valutazioni soggettive di adattivitÃ 

---

### ğŸ“Š Analisi Qualitativa
Categorizza le risposte aperte in base a:
- â€œ**Troppo semplice**â€ o â€œ**troppo tecnica**â€
- â€œ**Adatta al mio livello**â€
- â€œ**Vorrei piÃ¹ esempi/metafore**â€

---

### ğŸ“Œ Interpretazione
- **Alto allineamento** spiegazione â†” profilo â†’ TARS Ã¨ adattivo
- **Scarsa corrispondenza** â†’ generazione troppo generica o non personalizzata
- Effetti piÃ¹ evidenti nei **novizi** â†’ maggiore beneficio della personalizzazione

---

### ğŸ“¤ Output Attesi
- Matrice: `Profilo utente` â†” `Caratteristiche spiegazione`
- Grafico di dispersione: esperienza vs adattivitÃ  percepita
- **Quote qualitative** rappresentative (per illustrare ciascuna categoria)

---

In tommy Sono stati applicati due modelli standard per la valutazione:
- Il Technology Acceptance Model (TAM) per misurare l'utilitÃ  percepita e la facilitÃ  d'uso percepita.
- Il Task Load Index (NASA-TLX) per valutare il carico cognitivo.
- Feedback Qualitativo Aperto: Oltre alle metriche quantitative, Ã¨ stato raccolto feedback qualitativo aperto dopo il completamento di ciascun compito. Questo feedback Ã¨ stato poi analizzato utilizzando il metodo del card sorting. Questo tipo di feedback non deriva da domande strutturate predefinite, ma da commenti liberi degli utenti.
- ho notato che per tommy hanno fatto prima uno screening dei soggetti, dovremmo farlo anche noi?
- in tommy c'erano 15 persone